{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lq_5-q3Awfnw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import fsspec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDLdugn4wfn0"
      },
      "outputs": [],
      "source": [
        "root_dir = 'abfss://lakebeehaven@beehaven.dfs.core.windows.net/'\n",
        "source = root_dir+'silver/processing/'\n",
        "\n",
        "silver_path = root_dir + 'silver/'\n",
        "\n",
        "gold_processing = root_dir+'gold/processing/'\n",
        "\n",
        "\n",
        "# You will find your account name and key here:\n",
        "# datalake storage account -> Security+Networking -> Access keys \n",
        "storage_options = {\n",
        "    \"account_name\": \"beehaven\",\n",
        "    \"account_key\": \"aKey\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ4vkJxBwfn1",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Read the csv files from Silver/processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrIyf0fVwfn2"
      },
      "outputs": [],
      "source": [
        "# Filesystem with storage options\n",
        "fs = fsspec.filesystem('abfss', account_name='beehaven')\n",
        "\n",
        "# Use full path inside the container\n",
        "csv_files = fs.glob('abfss://lakebeehaven@beehaven.dfs.core.windows.net/silver/processing/*.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nuPY_VUwfn2",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "csv_files[0].split('/')[-1]#.split('.')[0].split('_')[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvFUbq1Lwfn3",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# Filesystem with storage options\n",
        "fs = fsspec.filesystem('abfss', account_name='beehaven')\n",
        "\n",
        "# Use full path inside the container\n",
        "csv_files = fs.glob('abfss://lakebeehaven@beehaven.dfs.core.windows.net/silver/processing/*.csv')\n",
        "\n",
        "# Store the locations in a list\n",
        "locations = []\n",
        "for name in csv_files:\n",
        "    locations.append(name.split('/')[-1].split('.')[0].split('_')[1])\n",
        "\n",
        "\n",
        "# Read each CSV into a dictionary using the filename as the key\n",
        "dfs = {}\n",
        "for file in csv_files:\n",
        "\n",
        "    # Extract the name without folder\n",
        "    name = file.split('/')[-1]\n",
        "    dfs[name.split('.')[0]] = pd.read_csv(source + name)\n",
        "    #dfs[name.split('_')[0]] = pd.read_csv(source + name)\n",
        "\n",
        "# Automatically create variables which will store the dfs from dictionary keys\n",
        "for name, df in dfs.items():\n",
        "    globals()[name] = df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZMeO3g0wfn4",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create functions to clean the csvs and write them as parquete files to Silver"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNpJ9phhwfn4",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZmS3iIqwfn5",
        "outputId": "a2489992-4fa1-4f45-c072-f93a02a4bfcc"
      },
      "outputs": [],
      "source": [
        "def clean_flow(flow, place):\n",
        "\n",
        "    # Convert the timestamp to dtype data type\n",
        "    flow['timestamp'] = pd.to_datetime(flow['timestamp'])\n",
        "\n",
        "    # looks like all counts in one direction are listed ordered by timestamp\n",
        "    # then all counts in the other direction are listed ordered by timestamp\n",
        "    # split the flow into two sets\n",
        "    departures = flow.iloc[:flow.shape[0]//2].copy()\n",
        "    arrivels = flow.iloc[flow.shape[0]//2:].copy()\n",
        "\n",
        "    # because local time changes with season, convert to UTC\n",
        "    departures['timestamp'] = departures['timestamp'].dt.tz_localize('Europe/Berlin', ambiguous='infer').dt.tz_convert('UTC')\n",
        "    arrivels['timestamp'] = arrivels['timestamp'].dt.tz_localize('Europe/Berlin', ambiguous='infer').dt.tz_convert('UTC')\n",
        "\n",
        "    # Reasambly. Merge flow figures back into one dataframe\n",
        "    flow_cl = departures.merge(arrivels, on='timestamp', suffixes=('_out', '_in'))\n",
        "\n",
        "    # write the data\n",
        "    flow_cl.to_parquet(f'{silver_path}/flow/{place}__{pd.Timestamp.now().strftime(\"%Y-%m-%dT%Hh%Mm%Ss.%f\")}.parquet', storage_options=storage_options, index=False)\n",
        "    flow_cl.to_parquet(f'{gold_processing}{place}_flow__{pd.Timestamp.now().strftime(\"%Y-%m-%dT%Hh%Mm%Ss.%f\")}.parquet', storage_options=storage_options, index=False)\n",
        "\n",
        "    return flow_cl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEWpPn67wfn5",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Humidity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzut_w-6wfn6",
        "outputId": "c9863cdb-3438-4d37-c217-9bc5e2b9ac3e"
      },
      "outputs": [],
      "source": [
        "def clean_humidity(humidity, place):\n",
        "\n",
        "    # Convert the timestamp to dtype data type\n",
        "    humidity['timestamp'] = pd.to_datetime(humidity['timestamp'])\n",
        "\n",
        "    # humidity values cannot be negative\n",
        "    humidity['humidity'] = humidity['humidity'].abs()\n",
        "\n",
        "    # convert to UTC\n",
        "    # any ambiguous times ARE in daylight saving (summer time)\n",
        "    humidity['timestamp'] = humidity['timestamp'].dt.tz_localize(\"Europe/Berlin\", ambiguous=True).dt.tz_convert(\"UTC\")\n",
        "\n",
        "    # drop null values\n",
        "    #humidity = humidity.dropna(subset=\"humidity\")\n",
        "\n",
        "    # drop duplicates\n",
        "    humidity = humidity.drop_duplicates().copy()\n",
        "\n",
        "    # write the data\n",
        "    humidity.to_parquet(f'{silver_path}/humidity/{place}__{pd.Timestamp.now().strftime(\"%Y-%m-%dT%Hh%Mm%Ss.%f\")}.parquet', storage_options=storage_options, index=False)\n",
        "    humidity.to_parquet(f'{gold_processing}{place}_humidity__{pd.Timestamp.now().strftime(\"%Y-%m-%dT%Hh%Mm%Ss.%f\")}.parquet', storage_options=storage_options, index=False)\n",
        "\n",
        "    return humidity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3074Wfwgwfn6",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwtJcCVJwfn6",
        "outputId": "3c39fba4-5f48-46bb-8650-ec383faeb11c"
      },
      "outputs": [],
      "source": [
        "def clean_temperature(temperature, place):\n",
        "\n",
        "    # Convert the timestamp to dtype data type\n",
        "    temperature['timestamp'] = pd.to_datetime(temperature['timestamp'])\n",
        "\n",
        "    # convert to UTC\n",
        "    temperature['timestamp'] = temperature['timestamp'].dt.tz_localize(\"Europe/Berlin\", ambiguous=\"infer\").dt.tz_convert(\"UTC\")\n",
        "\n",
        "    # drop null values\n",
        "    #temperature = temperature.dropna(subset=\"temperature\")\n",
        "\n",
        "    # drop duplicates\n",
        "    temperature = temperature.drop_duplicates().copy()\n",
        "\n",
        "    # write the data\n",
        "    temperature.to_parquet(f'{silver_path}/temperature/{place}__{pd.Timestamp.now().strftime(\"%Y-%m-%dT%Hh%Mm%Ss.%f\")}.parquet', storage_options=storage_options, index=False)\n",
        "    temperature.to_parquet(f'{gold_processing}{place}_temperature__{pd.Timestamp.now().strftime(\"%Y-%m-%dT%Hh%Mm%Ss.%f\")}.parquet', storage_options=storage_options, index=False)\n",
        "\n",
        "    return temperature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBcDq6Y0wfn6",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-QbIPDAwfn6",
        "outputId": "93fa45a5-c72c-41ee-d449-6a6794b2b1ea"
      },
      "outputs": [],
      "source": [
        "def clean_weight(weight, place):\n",
        "\n",
        "    # Convert the timestamp to dtype data type\n",
        "    weight['timestamp'] = pd.to_datetime(weight['timestamp'])\n",
        "\n",
        "    # weight values cannot be negative\n",
        "    weight['weight'] = weight['weight'].abs()\n",
        "\n",
        "    # convert to UTC\n",
        "    # any ambiguous times ARE in daylight saving (summer time)\n",
        "    weight['timestamp'] = weight['timestamp'].dt.tz_localize(\"Europe/Berlin\", ambiguous=True).dt.tz_convert(\"UTC\")\n",
        "\n",
        "    # drop null values\n",
        "    #weight = weight.dropna(subset=\"weight\")\n",
        "\n",
        "    # drop duplicates\n",
        "    weight = weight.drop_duplicates().copy()\n",
        "\n",
        "    # write the data\n",
        "    weight.to_parquet(f'{silver_path}/weight/{place}__{pd.Timestamp.now().strftime(\"%Y-%m-%dT%Hh%Mm%Ss.%f\")}.parquet', storage_options=storage_options, index=False)\n",
        "    weight.to_parquet(f'{gold_processing}{place}_weight__{pd.Timestamp.now().strftime(\"%Y-%m-%dT%Hh%Mm%Ss.%f\")}.parquet', storage_options=storage_options, index=False)\n",
        "\n",
        "    return weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Av8rFKgwfn7",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Weather"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "Nw0-pUd1wfn7",
        "outputId": "de0ee2c1-30c5-4e2e-b40d-11647f813c46",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def get_weater_data(place):\n",
        "\n",
        "    import requests\n",
        "    import json\n",
        "\n",
        "\n",
        "    # get the timestamps from the dfs dictionary\n",
        "    timestamps = [df[\"timestamp\"] for df in dfs.values() if \"timestamp\" in df.columns]\n",
        "\n",
        "    # convert them to UTC\n",
        "    timestamps = [\n",
        "    ts.dt.tz_convert('UTC') if ts.dt.tz is not None else ts.dt.tz_localize('UTC')\n",
        "    for ts in timestamps]\n",
        "\n",
        "    start_date = min(ts.min() for ts in timestamps)\n",
        "    end_date = max(ts.max() for ts in timestamps)\n",
        "\n",
        "\n",
        "    raw_sink = root_dir+\"bronze/archive/weather/\"\n",
        "\n",
        "    coords = {\n",
        "        \"schwartau\": {\"lat\": 53.919444, \"lon\": 10.6975},\n",
        "        \"wurzburg\": {\"lat\": 49.783333, \"lon\": 9.933333}\n",
        "    }\n",
        "    measures_to_ignore = [\"timestamp\", \"source_id\", \"condition\", \"precipitation_probability\",\n",
        "                        \"precipitation_probability_6h\", \"icon\", \"fallback_source_ids\"]\n",
        "\n",
        "    url = \"https://api.brightsky.dev/weather\"\n",
        "    headers = {\"Accept\": \"application/json\"}\n",
        "    params = coords[place] | {\"date\": start_date, \"last_date\": end_date}\n",
        "\n",
        "    response = requests.get(url, headers=headers, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        weather = response.json()\n",
        "\n",
        "        with fsspec.open(\n",
        "        raw_sink + f\"{place}__{start_date.strftime('%Y-%m-%d')}-{end_date.strftime('%Y-%m-%d')}.json\",\n",
        "        \"w\",\n",
        "        **storage_options\n",
        "    ) as f:\n",
        "            json.dump(weather, f, indent=4)\n",
        "\n",
        "\n",
        "\n",
        "        # with open (raw_sink+f\"schwartau__{start_date.strftime('%Y-%m-%d')}-{end_date.strftime('%Y-%m-%d')}.json\", \"w\") as f:\n",
        "        #     json.dump(weather, f, indent=4)\n",
        "\n",
        "        sources_df = pd.DataFrame(weather[\"sources\"])\n",
        "        for time in weather[\"weather\"]:\n",
        "            temp_dict = {}\n",
        "            for measure in time.keys():\n",
        "                if measure in measures_to_ignore:\n",
        "                    continue\n",
        "                if measure in time.get(\"fallback_source_ids\",{}):\n",
        "                    temp_dict[f\"{measure}_source_distance\"] = sources_df.loc[sources_df[\"id\"] == time.get(\"fallback_source_ids\",{}).get(measure), \"distance\"].values[0]\n",
        "                else:\n",
        "                    temp_dict[f\"{measure}_source_distance\"] = sources_df.loc[sources_df[\"id\"] == time[\"source_id\"], \"distance\"].values[0]\n",
        "            time.update(temp_dict)\n",
        "\n",
        "        weather_df = (\n",
        "            pd.DataFrame(weather[\"weather\"])\n",
        "            .drop([\"source_id\", \"visibility\", \"condition\", \"icon\", \"precipitation_probability\", \"precipitation_probability_6h\", \"fallback_source_ids\"], axis=1)\n",
        "        )\n",
        "        weather_df[\"timestamp\"] = pd.to_datetime(weather_df[\"timestamp\"])\n",
        "    else:\n",
        "        print(f\"Failure to gather weather data for period {start_date} to {end_date}.\\nReason: {response.text}\")\n",
        "\n",
        "\n",
        "    # write the data\n",
        "    processed_sink = root_dir+'silver/weather/'\n",
        "    weather_df.to_parquet(f'{processed_sink}{place}__{pd.Timestamp.now().strftime(\"%Y-%m-%dT%Hh%Mm%Ss.%f\")}.parquet', storage_options=storage_options, index=False)\n",
        "    weather_df.to_parquet(f'{gold_processing}{place}_weather__{pd.Timestamp.now().strftime(\"%Y-%m-%dT%Hh%Mm%Ss.%f\")}.parquet', storage_options=storage_options, index=False)\n",
        "\n",
        "    return weather_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I0ylJGvwfn7",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Clean and Write the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jruKPmNswfn7",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "e79f27eb-0cbd-4fef-b338-ff946a1523b4"
      },
      "outputs": [],
      "source": [
        "if \"schwartau\" in locations:\n",
        "    clean_flow(flow_schwartau, \"schwartau\")\n",
        "    clean_humidity(humidity_schwartau, \"schwartau\")\n",
        "    clean_temperature(temperature_schwartau, \"schwartau\")\n",
        "    clean_weight(weight_schwartau, \"schwartau\")\n",
        "    get_weater_data(\"schwartau\")\n",
        "else:\n",
        "    print(\"Warning: 'schwartau' is missing in locations.\")\n",
        "\n",
        "if \"wurzburg\" in locations:\n",
        "    clean_flow(flow_wurzburg, \"wurzburg\")\n",
        "    clean_humidity(humidity_wurzburg, \"wurzburg\")\n",
        "    clean_temperature(temperature_wurzburg, \"wurzburg\")\n",
        "    clean_weight(weight_wurzburg, \"wurzburg\")\n",
        "    get_weater_data(\"wurzburg\")\n",
        "else:\n",
        "    print(\"Warning: 'wurzburg' is missing in locations.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tJQ_Lgywfn7",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Release Spark pool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbUIB7_0wfn7",
        "outputId": "0c11b6ed-9398-4d8c-d0ec-7cfcf5c726aa"
      },
      "outputs": [],
      "source": [
        "mssparkutils.session.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
